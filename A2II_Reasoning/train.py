import logging
import torch
import torch.utils.data as Data
from torch.optim import AdamW
import argparse
import os
import numpy as np
from tqdm import tqdm, trange
import random
# pip install accelerate
from transformers import T5Tokenizer, T5ForConditionalGeneration
from CoT_Model import MyFlanT5
from DataProcessor_first_rel import MyDataset
from sklearn.metrics import precision_recall_fscore_support
from tool import warmup_linear,parse_sequences,compute_metrics
import json

def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', default='twitter2015', type=str, help=', ')
    parser.add_argument('--data_dir', default="/data/lzy1211/code/A2II/instructBLIP/CoT_data_process/CoT_data", type=str)
    parser.add_argument('--img_feat_dir', default="/data/lzy1211/code/A2II/instructBLIP/img_data", type=str)
    parser.add_argument('--output_dir', default="/data/lzy1211/code/A2II/instructBLIP/results/A2II-Reasoning/", type=str)
    parser.add_argument('--gradient_accumulation_steps', default=4, type=int)
    parser.add_argument('--BATCH_SIZE', default=16, type=int)
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--EPOCHS', default=2, type=int)
    parser.add_argument('--LEARNING_RATE', default=1e-5, type=float)
    parser.add_argument("--warmup_proportion",
                    default=0.1,
                    type=float,
                    help="Proportion of training to perform linear learning rate warmup for. "
                            "E.g., 0.1 = 10%  of training.")
    parser.add_argument("--max_seq_len",
                        default=128,
                        type=int,
                        help="The maximum total input sequence length after WordPiece tokenization. \n"
                             "Sequences longer than this will be truncated, and sequences shorter \n"
                             "than this will be padded.")
    parser.add_argument("--run_name",
                        default="pytorch_model",
                        type=str,
                        help="运行的名称")
    parser.add_argument("--output_model_name",
                        default="pytorch_model",
                        type=str,
                        help="保存权重的名称")
    parser.add_argument("--output_log_name",
                        default="log",
                        type=str,
                        help="日志的名称")  
    opt = parser.parse_args()

    return opt


def post_dataloader(batch):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    input_ids,input_attention_mask,input_hidden_states,input_pooler_outputs,labels,senti_label=batch
      
    input_ids = input_ids.clone().detach().long().to(device)
    input_attention_mask = input_attention_mask.clone().detach().float().to(device)
    
    labels=labels.to(device).long()
    senti_label=senti_label.to(device).long()
    
    input_pooler_outputs = input_pooler_outputs.clone().detach().to(input_ids.device)
    input_hidden_states = input_hidden_states.clone().detach().to(input_ids.device)
      
    return input_ids,input_attention_mask,input_hidden_states,input_pooler_outputs,labels,senti_label

 
def warmup_linear(x, warmup=0.002):
    if x < warmup:
        return x/warmup
    return 1.0 - x

def evaluate(args,model,test_dataloader, type='Dev'):
    model.eval()
    pred_sequence,senti_labels=[],[]
    for batch in tqdm(test_dataloader, desc="Testing"):
        input_ids,input_attention_mask,input_hidden_states,input_pooler_outputs,labels,senti_label= post_dataloader(batch)
        with torch.no_grad():
            outputs= model.generate(input_ids=input_ids,
                            attention_mask=input_attention_mask,              
                            input_hidden_states    = input_hidden_states,
                            labels=labels)
            pred_sequence.extend(outputs)
            senti_labels.extend(senti_label.detach().cpu().numpy())

    senti_preds=parse_sequences(pred_sequence)
    senti_result = compute_metrics(senti_preds, senti_labels)
    result = {     
        f'senti_acc':senti_result['acc'],    
        f'senti_f1':senti_result['f1'],  
   
    }
    # if type=='Test':
    #     #   保存预测的结果
    #     pred_res=[]
    #     sentiment_map = {'0': 'neutral', '1': 'positive', '2': 'negative'}
    #     for pred,gold in zip(pred_sequence,senti_labels):
    #         item={
    #             'pred':pred,
    #             'gold': sentiment_map[str(gold)]
    #         }
    #         pred_res.append(item)
    #     with open(args.output_test_path,'w',encoding='utf-8') as f:
    #         json.dump(pred_res,f)   
    return result



def train(args,model,train_dataset,val_dataset,optimizer_t5,logger,):

    max_senti_acc = 0.0
    best_epoch=-1
    Rel_global_step,global_step = 0,0
    train_number=train_dataset.number
    num_train_steps = int( train_number / args.BATCH_SIZE * args.EPOCHS)
    train_dataloader= Data.DataLoader(dataset=train_dataset,shuffle=True, batch_size=args.BATCH_SIZE,num_workers=0)
    val_dataloader= Data.DataLoader(dataset=val_dataset,shuffle=True, batch_size=args.BATCH_SIZE,num_workers=0)

    # # train
    logger.info("*************** Running training ***************")
    for train_idx in trange(int(args.EPOCHS), desc="Epoch"):
        logger.info("************************************************** Epoch: "+ str(train_idx) + " *************************************************************")
        logger.info("  Num examples = %d",  train_number) 
        logger.info("  Batch size = %d", args.BATCH_SIZE)
        logger.info("  Num steps = %d", num_train_steps)
        
        ### train
        model.train()
        senti_l,rel_l=0,0
        for step,data in enumerate(tqdm(train_dataloader,desc="Iteration")):
            batch=data
            # Sentiment
            input_ids,input_attention_mask,input_hidden_states,input_pooler_outputs,labels,senti_label= post_dataloader(batch)
            outputs= model(input_ids=input_ids,
                            input_attention_mask=input_attention_mask,              
                            input_hidden_states    = input_hidden_states,
                            labels=labels)
            output_loss=outputs['loss']
            output_loss.backward()
            lr_this_step = args.LEARNING_RATE * warmup_linear(global_step/num_train_steps, args.warmup_proportion)
            for param_group in optimizer_t5.param_groups:
                param_group['lr'] = lr_this_step
            optimizer_t5.step()
            optimizer_t5.zero_grad()
            global_step += 1
            senti_l+=output_loss.item()

        senti_l=senti_l/global_step
        logger.info("sentiment_loss:%s",senti_l)
        # wandb.log(
        #     {
        #         "train_loss":senti_l
        #     })
        
        # Dev evaluation
        model.eval()
        logger.info("***** Running evaluation on Dev Set*****")
        logger.info("  SA Num examples = %d", val_dataset.number) #len(eval_examples)
        logger.info("  Batch size = %d", args.BATCH_SIZE)
        dev_result=evaluate(args,model,val_dataloader)
        logger.info("***** Dev Eval results *****")
        for key in sorted(dev_result.keys()):
            logger.info("  %s = %s", key, str(dev_result[key]))
 
        # save model
        if dev_result['senti_acc'] >= max_senti_acc:
            max_senti_acc=dev_result['senti_acc']
            best_epoch=train_idx
            # Save a trained model
            model_to_save = model.module if hasattr(model, 'module') else model  
            torch.save(model_to_save.state_dict(), args.output_model_file)
            logger.info("best_epoch: %d",best_epoch)

    logger.info("best_epoch: %d",best_epoch)


def main(args):
    model_path='/data/lzy1211/code/model/flan-t5-base/'
    args.model_path=model_path
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    args.output_model_file = os.path.join(args.output_dir, f"{args.output_model_name}.bin")
    args.output_test_path=os.path.join(args.output_dir, f"{args.dataset}.json")
    output_logger_file=os.path.join(args.output_dir,f'{args.output_log_name}.txt')
    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                datefmt = '%m/%d/%Y %H:%M:%S',
                level = logging.INFO,
                filename=output_logger_file)
    logger = logging.getLogger(__name__)

    tokenizer = T5Tokenizer.from_pretrained(model_path)
    tokenizer.add_tokens(
        ['<image>', '</image>', '<explain>', '</explain>', '<i_explain>', '</i_explain>', '<emotion>', '</emotion>',
         'qa: ', 'qea: ', 'qiea: '])
    args.tokenizer = tokenizer
    t5_model=T5ForConditionalGeneration.from_pretrained(model_path)
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    # logger.info(args)
    

    # train_data=f"{args.data_dir}/{args.dataset}/new_train.json"
    # train_img_data=f"{args.img_feat_dir}/{args.dataset}/train.pkl"

    # val_data=f"{args.data_dir}/{args.dataset}/new_val.json"
    # val_img_data=f"{args.img_feat_dir}/{args.dataset}/val.pkl"

    # test_data=f"{args.data_dir}/{args.dataset}/new_test.json"
    # test_img_data=f"{args.img_feat_dir}/{args.dataset}/test.pkl"

    train_data=f"{args.data_dir}/train.json"
    val_data=f"{args.data_dir}/dev.json"
    test_data=f"{args.data_dir}/test.json"
    img_data=f"{args.img_feat_dir}/twitter2017/train.pkl"
    reason_data='/data/lzy1211/code/A2II/instructBLIP/reason_data/data/twitter2017/train_cause.json'
    caption_data='/data/lzy1211/code/A2II/instructBLIP/reason_data/data/twitter2017/captions.json'
    train_dataset=MyDataset(args,train_data,img_data,reason_data,caption_data,tokenizer)
    val_dataset=MyDataset(args,val_data,img_data,reason_data,caption_data,tokenizer)
    test_dataset=MyDataset(args,test_data,img_data,reason_data,caption_data,tokenizer)

    model = MyFlanT5(args=args,model=t5_model,tokenizer=tokenizer)
    model.to(device)

    # Prepare optimizer
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
        ]
    optimizer_t5 = AdamW(optimizer_grouped_parameters,
                            lr=args.LEARNING_RATE)
   
    # train
    train(args,model,train_dataset,val_dataset,optimizer_t5,logger)
    
    # Test
    model.eval()
    logger.info("***** Running evaluation on Test Set*****")
    logger.info("  Num examples = %d", test_dataset.number) #len(eval_examples)
    logger.info("  Batch size = %d", args.BATCH_SIZE)
    test_dataloader= Data.DataLoader(dataset=test_dataset,shuffle=True, batch_size=args.BATCH_SIZE,num_workers=0)
    # Load a trained model that you have fine-tuned
    model_state_dict = torch.load(args.output_model_file)
    model.load_state_dict(model_state_dict)
    test_results=evaluate(args,model,test_dataloader,type='Test')
    logger.info("***** Test Eval results *****")
    for key in sorted(test_results.keys()):
        logger.info("  %s = %s", key, str(test_results[key]))




if __name__ == "__main__":
   
    args=get_parser()
    main(args)